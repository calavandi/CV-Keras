# -*- coding: utf-8 -*-
"""Assignment_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZ1ajTnzEUnrz_k0gsKJBX-oW7-qDBY8
"""

from keras import backend as K
import time
import matplotlib.pyplot as plt
import numpy as np
# % matplotlib inline
np.random.seed(2017) 
from keras.models import Sequential
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers import Activation, Flatten, Dense, Dropout
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils

from keras.datasets import cifar10
(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()
num_train, img_channels, img_rows, img_cols =  train_features.shape
num_test, _, _, _ =  test_features.shape
num_classes = len(np.unique(train_labels))

class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']
fig = plt.figure(figsize=(8,3))
for i in range(num_classes):
    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])
    idx = np.where(train_labels[:]==i)[0]
    features_idx = train_features[idx,::]
    img_num = np.random.randint(features_idx.shape[0])
    im = features_idx[img_num]
    ax.set_title(class_names[i])
    plt.imshow(im)
plt.show()

def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # summarize history for accuracy
    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])
    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)
    axs[0].legend(['train', 'val'], loc='best')
    # summarize history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()

def accuracy(test_x, test_y, model):
    result = model.predict(test_x)
    predicted_class = np.argmax(result, axis=1)
    true_class = np.argmax(test_y, axis=1)
    num_correct = np.sum(predicted_class == true_class) 
    accuracy = float(num_correct)/result.shape[0]
    return (accuracy * 100)

train_features = train_features.astype('float32')/255
test_features = test_features.astype('float32')/255
# convert class labels to binary class labels
train_labels = np_utils.to_categorical(train_labels, num_classes)
test_labels = np_utils.to_categorical(test_labels, num_classes)

#base model
model = Sequential()
model.add(Convolution2D(48, 3, 3, border_mode='same', input_shape=(32, 32, 3)))
model.add(Activation('relu'))
model.add(Convolution2D(48, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(96, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(96, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(192, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(192, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

datagen = ImageDataGenerator(zoom_range=0.0, 
                             horizontal_flip=False)


start = time.time()
# Train the model
model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
                                 samples_per_epoch = train_features.shape[0], nb_epoch = 100, 
                                 validation_data = (test_features, test_labels), verbose=1)
end = time.time()
print ("Model took %0.2f seconds to train"%(end - start))
# plot model history
plot_model_history(model_info)
# compute test accuracy
print ("Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model))

"""###redefined model"""

model = Sequential()
model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(32, 32, 3)))
model.add(Activation('relu')) #30x30
model.add(BatchNormalization())

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))  #28x28
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(MaxPooling2D(pool_size=(2, 2)))#15x15

model.add(Convolution2D(128, 3, 3, border_mode='same'))  #15x15
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))



model.add(Convolution2D(128, 3, 3, border_mode='same')) #15x15
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25)) #5x5


model.add(Convolution2D(128, 3, 3, border_mode='same')) #15x15
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(MaxPooling2D(pool_size=(2, 2))) #8x8

model.add(Convolution2D(64, 3, 3))  #5x5
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Convolution2D(64, 3, 3))  #3x3
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Convolution2D(32, 3, 3))  #1x3
model.add(Activation('relu'))


model.add(Convolution2D(10, 1,1))
model.add(Activation('relu'))

model.add(Flatten())
model.add(Activation('softmax'))

print(num_classes)


# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint
from google.colab import drive

datagen = ImageDataGenerator(zoom_range=0.0, 
                             horizontal_flip=False)
drive.mount('/content/gdrive')

root_path = 'gdrive/My Drive/Colab_Models/'

checkpoint = ModelCheckpoint(root_path+'model_assignment_7.best-accuracy.hdfs', save_best_only=True,monitor='val_acc')
callback_list=[checkpoint]

start = time.time()
# Train the model
model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
                                 samples_per_epoch = train_features.shape[0], nb_epoch = 100, 
                                 validation_data = (test_features, test_labels), verbose=1, callbacks = callback_list)
end = time.time()
print ("Model took %0.2f seconds to train"%(end - start))
# plot model history
plot_model_history(model_info)
# compute test accuracy
print ("Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model))

"""### Model accuracy is 88.13% in the 97th epoch which is higher than the base accuracy."""

from keras.models import load_model

root_path = 'gdrive/My Drive/Colab_Models/'
model = load_model(root_path+'model_assignment_7.best-accuracy.hdfs')

from keras.preprocessing import image
import keras.backend as K
import numpy as np
import cv2
import sys


pred = model.predict(test_features[:10])
class_idx = np.argmax(pred[0])
class_output = model.output[:, class_idx]
last_conv_layer = model.get_layer("activation_14")

grads = K.gradients(class_output, last_conv_layer.output)[0]
pooled_grads = K.mean(grads)
iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])
pooled_grads_value, conv_layer_output_value = iterate([test_features[:10]])

model.summary()

print(conv_layer_output_value, (conv_layer_output_value[0]))

for i in range(3):
    conv_layer_output_value[i] *= pooled_grads_value

heatmap = np.mean(conv_layer_output_value, axis=-1)
heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)

from google.colab.patches import cv2_imshow

(Train_features, Train_labels), (Test_features, Test_labels) = cifar10.load_data()
imgs = []
#for i in range(0,200,66):
#  imgs.append(Test_features[i])
imgs = [Test_features[314], Test_features[315], Test_features[316], Test_features[317]]
  
for img in imgs:
  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
  heatmap = np.uint8(255 * heatmap)
  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
  superimposed_img = cv2.addWeighted(img.astype('float32'), 0.6, heatmap.astype('float32'), 0.4, 0)
  img = cv2.resize(img, (128, 128))
  cv2_imshow(img)
  superimposed_img = cv2.resize(superimposed_img, (128,128))
  cv2_imshow(superimposed_img)

#print("\n\n")

!wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py
from random_eraser import get_random_eraser

datagen = ImageDataGenerator(zoom_range=0.0, 
                             horizontal_flip=False)
drive.mount('/content/gdrive')

root_path = 'gdrive/My Drive/Colab_Models/'

checkpoint = ModelCheckpoint(root_path+'model_assignment_7_b.best-accuracy.hdfs', save_best_only=True,monitor='val_acc')
callback_list=[checkpoint]

start = time.time()
# Train the model
model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
                                 samples_per_epoch = train_features.shape[0], nb_epoch = 100, 
                                 validation_data = (test_features, test_labels), verbose=1, callbacks = callback_list)
end = time.time()
print ("Model took %0.2f seconds to train"%(end - start))
# plot model history
plot_model_history(model_info)
# compute test accuracy
print ("Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model))

model = load_model(root_path+'model_assignment_7_b.best-accuracy.hdfs')
pred = model.predict(test_features[:10])
class_idx = np.argmax(pred[0])
class_output = model.output[:, class_idx]
last_conv_layer = model.get_layer("activation_15")

grads = K.gradients(class_output, last_conv_layer.output)[0]
pooled_grads = K.mean(grads)
iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])
pooled_grads_value, conv_layer_output_value = iterate([test_features[:10]])


print(conv_layer_output_value, (conv_layer_output_value[0]))

for i in range(3):
    conv_layer_output_value[i] *= pooled_grads_value
    
heatmap = np.mean(conv_layer_output_value, axis=-1)
heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)

(Train_features, Train_labels), (Test_features, Test_labels) = cifar10.load_data()
imgs = []
for i in range(0,200,66):
  imgs.append(Test_features[i])
imgs = [Test_features[314], Test_features[315], Test_features[316], Test_features[317]]
  
for img in imgs:
  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
  heatmap = np.uint8(255 * heatmap)
  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
  superimposed_img = cv2.addWeighted(img.astype('float32'), 0.6, heatmap.astype('float32'), 0.4, 0)
  img = cv2.resize(img, (128, 128))
  cv2_imshow(img)
  superimposed_img = cv2.resize(superimposed_img, (128,128))
  cv2_imshow(superimposed_img)

#print("\n\n")

